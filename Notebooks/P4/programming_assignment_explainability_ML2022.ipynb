{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMSxpJAkqYzk"
      },
      "source": [
        "# Assignment 4: Explainability\n",
        "\n",
        "*Part of the course:\n",
        "Machine Learning (code: INFOB3ML), fall 2022, Utrecht University*\n",
        "\n",
        "Total points: 10 (100%)\n",
        "\n",
        "Deadline: Friday 4 November, 23:59\n",
        "\n",
        "**Write your names and student numbers here: ___**\n",
        "\n",
        "Submit one ipynb file per pair.\n",
        "\n",
        "**Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moyaViIx8WzS"
      },
      "source": [
        "## Installation\n",
        "\n",
        "For this assignment, we are going to use the following Python packages:\n",
        "\n",
        "matplotlib, pandas, statsmodels, interpret, scikit-learn, openpyxl and graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6EaC6P7RqXOh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'conda' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Installing packages\n",
        "!conda install python-graphviz\n",
        "!pip install matplotlib pandas statsmodels interpret sklearn openpyxl\n",
        "# Needed for PDP\n",
        "!pip install scikit-learn --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeSC0_WEpY0k"
      },
      "source": [
        "## Downloading the data\n",
        "We are going to use the combined cycle power plant dataset. This dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. We have the following features: hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V). We will train ML models to predict the net hourly electrical energy output (EP) of the plant.\n",
        "\n",
        "For a detailed description, see: [[Description](https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant)]\n",
        "\n",
        "We first need to download and prepare data. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fleSmPrE7UMT"
      },
      "outputs": [],
      "source": [
        "# Download and unzip data\n",
        "!wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/00294/CCPP.zip\n",
        "!unzip CCPP.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQpW5C3Sg9YA"
      },
      "source": [
        "## Loading and preprocessing the data\n",
        "We split the data into training (first 5000 instances) and validation (the subsequent 2000) and test (the last 2568) sets. We will use the training set to train a model, and validation set to optimize the model hyper-parameters. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JycjPmn_7p41"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# global variables\n",
        "DATA_FILENAME = 'CCPP/Folds5x2_pp.xlsx'\n",
        "FEATURE_NAMES = ['AT', 'V', 'AP', 'RH']\n",
        "LABEL_NAME = 'PE'\n",
        "# Load the data from the excel file\n",
        "def load_data():\n",
        "    def split_feature_label(data_set):\n",
        "        features = data_set[FEATURE_NAMES]\n",
        "        labels = data_set[LABEL_NAME]\n",
        "        return features, labels\n",
        "\n",
        "    data = pd.read_excel(DATA_FILENAME)\n",
        "    train_set, dev_set, test_set = data[:5000], data[5000: 7000], data[7000:]\n",
        "\n",
        "    train_features, train_labels = split_feature_label(train_set)\n",
        "    dev_features, dev_labels = split_feature_label(dev_set)\n",
        "    test_features, test_labels = split_feature_label(test_set)\n",
        "\n",
        "    return train_features, train_labels, dev_features, \\\n",
        "        dev_labels, test_features, test_labels\n",
        "\n",
        "\n",
        "# preprocess (by z-normalization) the data for the regression task\n",
        "# return the normalized feature sets and corresponding target variables \n",
        "def prepare_load_regression_data():\n",
        "    train_features, train_labels, dev_features, \\\n",
        "        dev_labels, test_features, test_labels = load_data()\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(train_features)\n",
        "    train_features = pd.DataFrame(data=scaler.transform(train_features), columns=FEATURE_NAMES)\n",
        "    dev_features = pd.DataFrame(data=scaler.transform(dev_features), columns=FEATURE_NAMES)\n",
        "    test_features = pd.DataFrame(data=scaler.transform(test_features), columns=FEATURE_NAMES)\n",
        "\n",
        "    return train_features, train_labels, dev_features, \\\n",
        "        dev_labels, test_features, test_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QabF2JOdMTI4"
      },
      "source": [
        "## Training and Interpreting a Linear Regression Model\n",
        "\n",
        "**Q1**. (10)% Train a linear regression model (we recommend the statsmodels package) and report $R^2$ (goodness of fit) statistic. \n",
        "\n",
        "For model interpretability, provide for each feature (+ the bias variable) the following in tabular format: \n",
        "* Weight estimates\n",
        "* SE (standard error of estimates) \n",
        "* T statistics \n",
        "\n",
        "\n",
        "Further Questions regarding the linear model (answers to be included in the notebook): \n",
        "\n",
        "**Q2**. (10%) Which three features are the most important?\n",
        "\n",
        "**Q3**. (10%) How does the gas turbine energy yield (EP) change with unit (one degree C) increase of the ambient temperature given that all other feature values remain the same? (Note: Here you should consider whether you use the original or z-normalized features to train your linear model.)\n",
        "\n",
        "**Q4**. (10%) Show bar graph illustrations of the feature effects for the first two validation set instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B91BszFhMStw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9294006278833447\n",
            "const    452.646673\n",
            "AT        -1.999857\n",
            "V         -0.225041\n",
            "AP         0.064599\n",
            "RH        -0.167095\n",
            "dtype: float64\n",
            "const    33.641497\n",
            "AT      -95.152274\n",
            "V       -22.543124\n",
            "AP        4.948008\n",
            "RH      -29.185187\n",
            "dtype: float64\n",
            "const    13.455010\n",
            "AT        0.021017\n",
            "V         0.009983\n",
            "AP        0.013056\n",
            "RH        0.005725\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# We recommend the statsmodels package\n",
        "import statsmodels.api as sm\n",
        "# Hint, by default this sm does not include the bias/offset term w_0\n",
        "# thus, you should add it yourself using sm.add_constant()\n",
        "\n",
        "# Linear regression\n",
        "train_features, train_labels, dev_features, dev_labels, test_features, test_labels = load_data()\n",
        "\n",
        "train_features = sm.add_constant(train_features)\n",
        "mod = sm.OLS(train_labels, train_features)\n",
        "results = mod.fit()\n",
        "results.summary()\n",
        "print(results.rsquared)\n",
        "print(results.params) # even checken of dit de juiste is\n",
        "print(results.tvalues)\n",
        "print(results.bse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tj6Pri4HBeO"
      },
      "source": [
        "Reflection: why would training a regression tree not work well for this dataset in terms of model interpretability? (answer in the notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7r09mMfeo2k"
      },
      "source": [
        "## Training and Interpreting an Explainable Boosting Model (Generalized Additive Model)\n",
        "**Q6**. (20%) Train a Explainable Boosting Machine (with [interpret.ml](https://github.com/interpretml/interpret/))\n",
        "\n",
        "(Note on grading: Training EBM 10%, global and local explanation visualizations - see below -  5% each)\n",
        "\n",
        "For a tutorial see: [[Tutorial](https://nbviewer.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Interpretable%20Regression%20Methods.ipynb)]\n",
        "\n",
        "\n",
        "* (5%) Visualize/provide global (model-wise) feature importances for EBM as a table or figure. What are the most important two features in EBM? Are they the same as in the linear model? \n",
        "* (5%) Visualize local (instance-wise) feature importances for a development set instance of your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u-ZmqpxweoZv"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<!-- http://127.0.0.1:7001/1942091090144/ -->\n",
              "<iframe src=\"http://127.0.0.1:7001/1942091090144/\" width=100% height=800 frameBorder=\"0\"></iframe>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyError",
          "evalue": "0",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Menee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[1;32mc:\\Users\\Menee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\Menee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [9], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m ebm_glob \u001b[39m=\u001b[39m ebm\u001b[39m.\u001b[39mexplain_global(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEBM\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m show(ebm_glob)\n\u001b[1;32m---> 10\u001b[0m ebm_local \u001b[39m=\u001b[39m ebm\u001b[39m.\u001b[39mexplain_local(dev_features[\u001b[39m0\u001b[39;49m], dev_labels[\u001b[39m0\u001b[39m], name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEBM\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m show(ebm_local)\n",
            "File \u001b[1;32mc:\\Users\\Menee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\Menee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 0"
          ]
        }
      ],
      "source": [
        "from interpret.glassbox import ExplainableBoostingRegressor\n",
        "from interpret import show\n",
        "\n",
        "\n",
        "ebm = ExplainableBoostingRegressor()\n",
        "ebm.fit(train_features, train_labels)\n",
        "ebm_glob = ebm.explain_global(name='EBM')\n",
        "show(ebm_glob)\n",
        "\n",
        "ebm_local = ebm.explain_local(dev_features, dev_labels, name='EBM')\n",
        "show(ebm_local)\n",
        "\n",
        "# EBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k7dAwTIfbsc"
      },
      "source": [
        "## Training and Explaining Neural Networks\n",
        "**Q7**. (20%) Train a Neural Network (using the training and validation sets): One-layer MLP (ReLU activation function + 50 hidden neurons) \n",
        "\n",
        "We recommend to use the Adam optimizer. Fine-tune the learning rate and any other hyper-parameters you find necessary. \n",
        "\n",
        "For a tutorial see: [[Tutorial](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)]\n",
        "\n",
        "Your code should report the results following the instructions below:\n",
        "\n",
        "Note on grading: training NN: 8%, answering below sub-questions 4% each.\n",
        "\n",
        "* (4%) Apply the trained neural network model on the test set and report Root Mean Square Error (RMSE) performance measure.\n",
        "\n",
        "* Analyzing factors influencing the neural network predictions. \n",
        "See the [Documentation](https://scikit-learn.org/stable/modules/partial_dependence.html) to use Partial Dependence Plot (PDP) and Independent Conditional Expectation (ICE) implementations in python.\n",
        "Use the trained one-layer MLP model to\n",
        "  * (4%) Generate and report a bivariate PDP using 'AT' (Ambient Temperature) and 'V' (Exhaust Vacuum) features (Note: not two univariate PDPs but one bivariate PDP).\n",
        "  * (4%) Generate  ICE plots for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQjg_qtCf_WD"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# One-layer MLP use  learning_rate_init=0.001 to get a reasonable model, optimize other parameters by experimentation\n",
        "# an RMSE around 4.2 is reasonable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpGv6J6XlBQ4"
      },
      "source": [
        "### Generating Model-Agnostic Explanations for NN predictions\n",
        "You can check the tutorials for LIME explanations for neural networks \n",
        "[[LIME Tutorial](https://nbviewer.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Explaining%20Blackbox%20Regressors.ipynb)]\n",
        "\n",
        "\n",
        "**Q8**. (10%) Provide explanations for two randomly selected test set instances using LIME for the trained NN model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIo-o_lClJYQ"
      },
      "outputs": [],
      "source": [
        "# Global explanations\n",
        "import graphviz\n",
        "from interpret import show\n",
        "\n",
        "# Local explanations (LIME)\n",
        "from interpret.blackbox import LimeTabular\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "b8fc55d22b506f310823b6bc9ecb5d19dc173fb1373e7bf8bb90a2220d0a6ca2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
