{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333fb0a5-1e6b-4211-8a64-2de9260378ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment 3 \n",
    "\n",
    "*Part of the course:\n",
    "Machine Learning (code: INFOB3ML), fall 2022, Utrecht University*\n",
    "\n",
    "Total points: 10 (100%)\n",
    "\n",
    "Deadline: Friday 21 October, 23:59\n",
    "\n",
    "**Write your names and student numbers here: ___**\n",
    "\n",
    "Submit one ipynb file per pair.\n",
    "\n",
    "**Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d823b0f",
   "metadata": {},
   "source": [
    "\n",
    "## The BoAW Approach for emotion classification of utterances\n",
    "In this programming assignment, you will be implementing Bag-of-Audio-Words (BoAW) method for acoustic low level descriptor (LLD) representation over utterances.\n",
    "The acoustic LLDs per utterance are extracted and provided to you in separate files.\n",
    "Your task is to implement the BoAW approach and the component methods, followed by classification using existing python libraries.\n",
    "\n",
    "In a nutshell, you will be implementing the following functions (use of an existing library/package like `sklearn` is not allowed for this part):\n",
    "1. Principal Component Analysis (PCA)\n",
    "2. K-Means algorithm \n",
    "3. BoAW representation function\n",
    "\n",
    "You will then experiment with different combinations of PCA dimensions (`p_pca`) and K-Means components (`K`), and optimize said combinations using a Support Vector Machine (SVM) classifier. \n",
    "You are given data of 16 speakers, where: \n",
    "- The data of the first 8 speakers will be used as training set\n",
    "- The data of speakers 9-12 will be used as validation set and\n",
    "- The data of speakers 13-16 will be used as the test set.\n",
    "\n",
    "The training set will be used to learn PCA and K-Means model, as well as to train the SVM classifiers using the BoAW representations as features and the emotions of the speakers as responses.\n",
    "The hyper-parameter `C` of the classifiers as well as the number of principal components (`p_pca`) and cluster components (`K`) will be optimized on the validation set.\n",
    "The top model (resulting from the best combination of `p_pca`, `K` and `C` hyper-parameters) will be applied to the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b971fcf-1488-4ef7-8495-a5202ccd024e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The BoAW approach\n",
    "In the BoAW approach (can also be applied to visual descriptors) we train a K-Means model using the combined LLDs from all trained set utterances.\n",
    "Since K-Means assumes no correlations in the data, and partly for reducing dimensionality, we first apply PCA to LLDs. \n",
    "The BoAW representation amounts to assigning each LLD to the nearest K-Means cluster and computing a K-dimensional histogram of the accumulated instances around each mean.\n",
    "This means that each utterance will be \"represented\" by a histogram, where each entry is the number of LLD sequences in the utterance that are assigned to a certain cluster.\n",
    "Normalization of the histogram (so that each entry is a frequency as opposed to an absolute number) is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3eb90-b7c0-472d-a700-bf1e0118e06e",
   "metadata": {},
   "source": [
    "### Note\n",
    "The last part of this assignment contains a lot of code that you do not need to (and should not) modify. However, it is important that you understand what is going on there, otherwise it will be difficult to fill-in the blanks when we ask you to.\n",
    "\n",
    "The only places where you need to write code say \"YOUR CODE HERE\" explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa876d63-0a6a-4565-a6a7-98b0765aaed9",
   "metadata": {},
   "source": [
    "### Required packages:\n",
    "- numpy\n",
    "- pandas\n",
    "- scipy\n",
    "- sklearn\n",
    "\n",
    "\n",
    "### Required Dataset\n",
    "You can download the zipped folder containing the extracted acoustic LLDs from subset of RAVDESS dataset from: \n",
    "\n",
    "https://surfdrive.surf.nl/files/index.php/s/EeJxsea0CfCn17S\n",
    "\n",
    "Unzip it in the same folder as this python notebook to ensure smooth working of the scripts we have prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6b014f-7574-4101-bc4c-463ac50f4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684adf47-70db-439f-a337-492e4e60721d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Implementing PCA (Graded 20%)\n",
    "Define a function `mypca(X)` with the following signature:\n",
    " - Input  `X`: N x D dimensional dataset\n",
    " - Output: `(W, lambda, mu)`\n",
    "     + `W`: D x D eigenvalue sorted PCA projection matrix (i.e. first column is the first principal component)\n",
    "     + `lambdas`: Dx1 vector of sorted eigenvalues (sorted in _descending_ order)\n",
    "     + `mu`: Dx1 the mean of X (should be calculated and removed from the data before subsequent projection using W)\n",
    "     \n",
    "Hints: \n",
    " 1. Recall that you should start by ensuring that the data has zero mean.\n",
    " 2. You may want to make use of the numpy function `np.cov`. Be careful with what matrix you feed into this function. (Extra hint: look at the description `rowvar` argument of this function in the documentation).\n",
    " 3. You will want to sort the eigenvalues in descending order, but numpy's sort method can only sort in ascending order. You may want to consider using `np.argsort` and using the fact that the ascending sorting indices for `-v` and the descending sorting indices for `v` are the same.\n",
    " 4. Other numpy functions you may want to use: `np.mean`, `np.linalg.eig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69095bc0-d3d2-48d0-8c94-8933ba894d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mypca(X):\n",
    "    #--- YOUR CODE HERE ---#\n",
    "    \n",
    "    return W, lambdas, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8cf8fe-7274-4d1e-8260-050ed4589ddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# Three features; four samples.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m W, lambdas, mu \u001b[38;5;241m=\u001b[39m \u001b[43mmypca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mlambdas = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambdas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [4], line 4\u001b[0m, in \u001b[0;36mmypca\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmypca\u001b[39m(X):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#--- YOUR CODE HERE ---#\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mW\u001b[49m, lambdas, mu\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(4, 3) # Three features; four samples.\n",
    "W, lambdas, mu = mypca(X)\n",
    "print(f\"W = {W}\")\n",
    "print(f\"\\nlambdas = {lambdas}\")\n",
    "print(f\"\\nmu = {mu}\")\n",
    "\n",
    "assert W.shape == (3, 3), \"Wrong shape of array W!\"\n",
    "assert len(lambdas) == 3, \"Wrong length of vector lambdas!\"\n",
    "assert len(mu) == 3, \"Wrong length of vector mu!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f454a5-2106-4e35-82c0-3e8778992b5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Implementing K-Means (Graded 40%)\n",
    "Define a function `mykmeans(X, K, maxiter=200, epsilon=0.0001)` with the following signature:\n",
    " - Input\n",
    "     + `X`: N x D dimensional dataset\n",
    "     + `K`: Hyper-parameter (number of clusters) of K-Means\n",
    "     + `maxiter`: maximum number of iterations (default 200)\n",
    "     + `epsilon`: algorithm will stop when error improvement < epsilon (or number of iterations surpasses `maxiter`) (default 200)\n",
    " - Output: `(M, A)`\n",
    "     + `M`: K x D dimensional matrix of means\n",
    "     + `A`: N x 1 dimensional cluster assignments\n",
    "     \n",
    "Notes:\n",
    " 1. It is not necessary but you may want to test your function in a simple example that is easy to visualize, like the ones in the book. In that case, you'll need to come up with it yourself. If you do so, I would advise doing so in a separate Python file and if you do it in this notebook _do not leave that extra code in, or you'll be penalized_.\n",
    " 2. You will need to compute distances between samples `X[n]` and mean vectors `M[k]`. You can use the function `dist` given below for that. Do not modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d3c25f-9380-4283-b066-9bfdc405d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "from scipy.spatial.distance import cdist\n",
    "# Hint: you will need this function in your K-Means (mykmeans) and BoAW (myboaw) function implementations\n",
    "def dist(X, M):\n",
    "    \"\"\"Computes the matrix with entries dist_matrix_nk = euclidean distance between X[n] and M[k] efficiently.\"\"\"\n",
    "    return cdist(X,M,metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41963bd4-ec5f-4bed-9d54-5c4d674b36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mykmeans(datapoints, K, maxiter=200, epsilon=0.0001):\n",
    "    np.random.seed(1)\n",
    "    N, D = datapoints.shape\n",
    "    # initialize model/output variables\n",
    "    initial_indices = np.random.choice(N, K) # sample random K out of N instances to use as initial means\n",
    "    centroids = datapoints[initial_indices, :] # (K, D) matrix\n",
    "    min_indices = np.zeros((N, 1))\n",
    "    error_log = np.zeros((maxiter,1)) # You can save the errors here, to then use for computing delta\n",
    "    iteration = 0\n",
    "    cont = 1 # set to 0 when convergence criteria is met\n",
    "\n",
    "    print(\"Running k-means...\")\n",
    "    while cont:\n",
    "        #--- YOUR CODE HERE ---#\n",
    "        distances = cdist(datapoints,centroids)\n",
    "        cluster_assignments = np.array([np.argmin(i) for i in distances]) # Calculate which cluster this point is closest to        \n",
    "\n",
    "        if iteration == 0: \n",
    "            delta = 100000 # Something very high\n",
    "        else:\n",
    "            # Lets calculate the mean relative difference as delta\n",
    "            sumofdiff = 0\n",
    "            for index,distance in enumerate(distances):\n",
    "                sumofdiff += distance                \n",
    "            delta = sumofdiff / distances.size # denk ik\n",
    "            # ^^ Error calculation\n",
    "            \n",
    "            \n",
    "        if (delta <= epsilon or iteration == maxiter):\n",
    "            cont = 0\n",
    "            print('done')\n",
    "        else: # heb deze else zelf toegevoegd, idk of hij hier hoort\n",
    "             centroids = datapoints[cluster_assignments==datapoint].mean(axis=0) # nieuwe centroids gebaseerd op mean\n",
    "            \n",
    "        assert not np.isnan(delta), \"delta is nan!\"\n",
    "            \n",
    "        print(f\"iteration: {iteration}, delta: {delta}\")\n",
    "        iteration = iteration + 1\n",
    "    \n",
    "    # Final update of the indices specifying the clusters for each sample\n",
    "    dist_matrix = 0 #--- YOUR CODE HERE ---#\n",
    "    min_indices = 0 #--- YOUR CODE HERE ---#\n",
    "    A = min_indices # Just renaming\n",
    "    \n",
    "    return centroids, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b4aabd-172c-400d-84aa-ef21e2c87c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running k-means...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datapoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;66;03m#  two features; 12 samples.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmykmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [7], line 32\u001b[0m, in \u001b[0;36mmykmeans\u001b[1;34m(datapoints, K, maxiter, epsilon)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# heb deze else zelf toegevoegd, idk of hij hier hoort\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m      centroids \u001b[38;5;241m=\u001b[39m datapoints[cluster_assignments\u001b[38;5;241m==\u001b[39m\u001b[43mdatapoint\u001b[49m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# nieuwe centroids gebaseerd op mean\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(delta), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta is nan!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, delta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datapoint' is not defined"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "np.random.seed(0)\n",
    "X = np.random.randint(1, 10, (12, 2)) #  two features; 12 samples.\n",
    "K = 3\n",
    "mykmeans(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46551e5b-d763-4cfe-afcb-b4adda405bb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Implementing Bag of Audio Words (BoAW) representation (Graded 20%)\n",
    "Define a function `myboaw(X_LLD, M)` which assigns the set of LLDs in the first argument to the nearest mean using the matrix of means given in the second argument.\n",
    "The function should have the following signature:\n",
    " - Input:\n",
    "     + `X_LLD`: N x D matrix of LLDs from each utterance, where N is the number of frames / instances and D is the (reduced) LLD dimensionality\n",
    "     + `M`: K x D matrix of means (code vectors) outputed from K-Means\n",
    " - Output:\n",
    "     + `boaw_rep`: K x 1 matrix with the _normalized_ boaw representation for each cluster k in each entry.\n",
    "     \n",
    "Hint:\n",
    " - After obtaining the K x 1 vector of unnormalized boaw representations (whose entries are integers), you can get the normalized version by simple dividing the unnormalized vector by the sum over all the entries of the unnormalized boaw vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4fe2e-c84d-4567-b115-be673d2429c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myboaw(X_LLD, M):\n",
    "    #--- YOUR CODE HERE ---#\n",
    "    return boaw_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a255b6-d146-40ee-a60d-1d10d0ee8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "np.random.seed(0)\n",
    "X = np.random.randint(1, 10, (12, 2)) #  two features; 12 samples.\n",
    "K = 3\n",
    "M, _ = mykmeans(X, K)\n",
    "myboaw(X, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e1941-d5ce-476d-b405-8eb10120c942",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4: BoAW representation in action (graded 20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b6340-cee4-4fdd-9f9a-1b8e7379fd94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4.1: Data ingestion (0% - we do it for you!)\n",
    "Read the data and place them in an appropriate data structure such that LLD sequence coming from each file (=utterance) is contained separately.\n",
    "\n",
    "NOTES:\n",
    " 1. Even though you do not need to write code for this part, make sure you understand what is going on, otherwise you will not comprehend the next steps.\n",
    " 2. I would advise you to put the RAVDESS directory in the directory where this notebook is being run from - otherwise you will need to modify the variable `base_folder` below, as is written there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8bbf65-3c7a-4e93-9ec4-d576db9bcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Each file will have some data and metadata we will want to record. We do so in using a class:\n",
    "class FileData():\n",
    "    def __init__(self, speakerid, emoid, LLDseq, LLDcount):\n",
    "        self.speakerid = speakerid\n",
    "        self.emoid = emoid\n",
    "        self.LLDseq = LLDseq\n",
    "        self.LLDcount = LLDcount\n",
    "        \n",
    "\n",
    "def ingest_ravdess_data(base_folder):\n",
    "    \"\"\"Build list of FileData instances, one per file/utterance.\"\"\"\n",
    "    LLD_folder = 'LLD_CSV_IS13/'\n",
    "    list_actors_dirs = sorted(glob.glob(os.path.join(base_folder, LLD_folder, 'Actor*')))\n",
    "    #print(list_actors_dirs)\n",
    "    \n",
    "    cnt = 0\n",
    "    LLD_data = [] # To be populated by instances of FileData\n",
    "        \n",
    "    # The metadata can be obtained from the provided metadata.csv or from the\n",
    "    # filename directly. We'll do the latter.\n",
    "\n",
    "    for actor_folder in list_actors_dirs:\n",
    "        list_actorLLDfiles = sorted(glob.glob(os.path.join(actor_folder, '*.csv')))\n",
    "        #print(list_actorLLDfiles)\n",
    "        for actor_file in list_actorLLDfiles:\n",
    "            #print(f\"file: {actor_file}\")\n",
    "            cnt = cnt + 1\n",
    "            tmp_data_og = pd.read_csv(actor_file, sep=';')\n",
    "            tmp_data = tmp_data_og.iloc[:, 2:]\n",
    "            # if cnt == 1: # Visualise data on first iteration\n",
    "            #     print(tmp_data.shape)\n",
    "            #     print(tmp_data.iloc[:5, :4])\n",
    "            #     cols = tmp_data.columns\n",
    "            #     #print(f\"Column names: {[\"\\n\\t\" + f\"{col}\" for col in cols]}\")\n",
    "            #     print(\"\\nColumns:\")\n",
    "            #     for col in cols:\n",
    "            #         print(\"\\t\" + f\"{col}\")\n",
    "            actor_file_wo_ext = os.path.splitext(actor_file)[0] # Remove extension from file name\n",
    "            filecodes = str.split(actor_file_wo_ext,'-') # list with the filecodes, for example ['03', '02', ...]\n",
    "            emoid = int(filecodes[2]) # the third filecode corresponds to the emotion id\n",
    "            #print(f\"emoid: {emoid}\")\n",
    "            speakerid = int(filecodes[-1]) # the last filecode corresponds to the actor/speaker id\n",
    "            LLD_data.append(FileData(speakerid, emoid, tmp_data, tmp_data.shape[0]))\n",
    "    assert len(LLD_data) == 704, \"Something went wrong! There are 704 csv files in the RAVDESS dataset, but len(LLD_data) != 704.\"\n",
    "    \n",
    "    return LLD_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7659890d-32a6-4986-8d97-4c18f6c29bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY MODIFY IF BASE-FOLDER IS DIFFERENT!\n",
    "base_folder = './RAVDESS/' # Assuming that the RAVDESS directory is in the directory where this notebook is being run from.\n",
    "LLD_data = ingest_ravdess_data(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf54247-4a1f-454e-b13a-110499fc72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "# In this part, each function does one of these things:\n",
    "#  1. Generate train data table with one LLDseq (corresponding to one audio frame) per row, using the first few speakers only.\n",
    "#  2. Generate the mask arrays (using the speakerids) that will be used in the rest of this notebook\n",
    "#        for splitting the datasets in train, val and test sets.\n",
    "#  3. Generate the response variable vectors for the train, val and test sets.\n",
    "\n",
    "def build_ravdess_train_dataset(LLD_data, n_speakers_train=8):\n",
    "    \"\"\"Builds RAVDESS dataset from list of FileData corresponding to the LLD of the utterances. \n",
    "    \n",
    "    Input:\n",
    "        LLD_data -- list of FileData instances, one for each utterance file.\n",
    "        \n",
    "    Output:\n",
    "        all_train_LLDsequences -- dataframe of shape (N_seqs=total number of LLD seqs, number of LLD features),\n",
    "                                    with one LLD sequence per row.\n",
    "        \n",
    "    \"\"\"\n",
    "    all_train_LLDsequences = pd.concat([file_data.LLDseq for file_data in LLD_data if file_data.speakerid <= n_speakers_train]) # Each LLDseq is one feature vector\n",
    "\n",
    "    return all_train_LLDsequences\n",
    "\n",
    "\n",
    "def generate_splitting_masks(LLD_data, n_speakers_train=8, n_speakers_val=4):\n",
    "    all_speakerids = np.array([file_data.speakerid for file_data in LLD_data])\n",
    "\n",
    "    # Masks to build dataset using speaker ids:\n",
    "    mask_train = all_speakerids <= n_speakers_train\n",
    "    mask_val = np.logical_and(n_speakers_train < all_speakerids, all_speakerids <= n_speakers_train + n_speakers_val)\n",
    "    mask_test = n_speakers_train + n_speakers_val < all_speakerids\n",
    "    \n",
    "    return mask_train, mask_val, mask_test, all_speakerids\n",
    "\n",
    "def prepare_ravdess_response_data(LLD_data, mask_train, mask_val, mask_test):\n",
    "    all_emoids = np.array([file_data.emoid for file_data in LLD_data])\n",
    "    \n",
    "    # Response's part of the dataset\n",
    "    emoids_train = all_emoids[mask_train]\n",
    "    emoids_val = all_emoids[mask_val]\n",
    "    emoids_test = all_emoids[mask_test]\n",
    "    \n",
    "    return emoids_train, emoids_val, emoids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8456d0-2638-4beb-9ad5-c2b5e4190f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "# Make 0.5-0.25-0.25 train-val-test split using speakerid\n",
    "# (i.e. 8 speakers for the train, 4 speakers for the val and 4 speakers for the test sets).\n",
    "mask_train, mask_val, mask_test, _ = generate_splitting_masks(LLD_data)\n",
    "emoids_train, emoids_val, emoids_test = prepare_ravdess_response_data(LLD_data, mask_train, mask_val, mask_test)\n",
    "LLDs_train = build_ravdess_train_dataset(LLD_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d139457-2616-4047-b226-c4c54af7aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST:\n",
    "print(LLDs_train.shape)\n",
    "print(emoids_train[:44]) # Look at some responses\n",
    "\n",
    "assert LLDs_train.shape[1] == 130, \"There should be 130 features.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a0d6f-dbd5-462f-b188-7f7e1ab048f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4.2: Learn PCA projection (5%)\n",
    "Learn a PCA projection (i.e. the eigenvectors and eigenvalues of the covariance matrix) using your own PCA implementation, namely `mypca`.\n",
    "\n",
    "NOTE:\n",
    "If there is a large variance across variances of the features then it is wise to z-normalize the data before applying PCA (using for example an instance of the `sklearn.preprocessing.StandardScaler` class).\n",
    "\n",
    "Running the code below shows that indeed there is a huge difference in variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba492d46-00bd-41cc-8bd1-1373b16e373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "data_var = LLDs_train.var()\n",
    "print(f\"Variances:\\n{'-'*5}\\n{data_var}\\n{'-'*5}\")\n",
    "#ratio_extremes = np.max(data_var)/np.min(data_var)\n",
    "var_of_variances = np.var(data_var)\n",
    "#print(ratio_extremes)\n",
    "print(f\"Variance of the variances: {var_of_variances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cfdf5f-01f8-4ee8-a9f4-af9bde4c1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You now need to Z-normalize X. Call the resulting variable X_zn. Hint: use sklearn.preprocessing.StandardScaler.\n",
    "#--- YOUR CODE HERE ---#\n",
    "\n",
    "# Finally, apply mypca to extract pca_W and pca_lambdas\n",
    "pca_W, pca_lambdas, _ = mypca(X_zn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772eaa8f-f79c-4eb9-8f06-b0d90b5693f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST:\n",
    "print(X_zn.shape)\n",
    "print(pca_W.shape)\n",
    "print(pca_lambdas.shape)\n",
    "\n",
    "assert X_zn.shape == (162621, 130), \"Something went wrong!\"\n",
    "assert pca_W.shape == (130, 130), \"Something went wrong!\"\n",
    "assert pca_lambdas.shape == (130,), \"Something went wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2f981-715a-4e26-8711-c534106b7c0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4.3: Applying PCA and K-means (5%)\n",
    "\n",
    "Project the training set using the first `p_pca` PCA components and train a K-Means model with K components.\n",
    "Using a number `p_pca` of principal components equal to 120 and a number of clusters `K` of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388eb09-6feb-4e12-85a6-01934402e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "p_pca = 120\n",
    "K = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdbb55-6b36-489e-ac12-3a9d45b09162",
   "metadata": {},
   "source": [
    "We already have the pre-processed (z-normalized) training LLD data.\n",
    "\n",
    "First, get PCA projection onto the (basis spanned by) the first p_pca eigenvectors.\n",
    "Call the resulting matrix `Z_LLD_train`.\n",
    "\n",
    "Hint: This function should not have more than a couple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba2f4a-7344-49de-93ac-62ac0f0921db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_pcs(X, pca_W, p_pca):\n",
    "    # Projecting coincides with matrix multiplication (if pca_W column vectors are normalized, which they are):\n",
    "    #--- YOUR CODE HERE ---#\n",
    "    return X_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422f878-82f9-4015-a9d5-a3fcf403589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "Z_LLD_train = project_onto_pcs(X_zn, pca_W, p_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb8861-4c72-4d3f-b1f1-01d9a22c35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "print(Z_LLD_train.shape)\n",
    "assert Z_LLD_train.shape == (162621, 120), \"Something went wrong! Shape of Z_LLD_train is incorrect!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79973c-bd64-4f32-aac1-38b9660e28cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "# We will use only every fourth frame/LLD sequence, to reduce the computation time.\n",
    "\n",
    "def subsample(X, n_subsample):\n",
    "    X_subsample = X[np.arange(X.shape[0], step=n_subsample), :] # Use every n_sample'th point\n",
    "    return X_subsample\n",
    "    \n",
    "n_subsample = 4 # Use every fourth frame\n",
    "Z_LLD_train_subsample = subsample(Z_LLD_train, n_subsample)\n",
    "print(Z_LLD_train_subsample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee099e-c742-4f8e-99fb-2ee1bb8402ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your kmeans implementation to the projected, subsampled, train data:\n",
    "np.random.seed(0)\n",
    "M, _ = #--- YOUR CODE HERE ---#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba6d6b-6cdf-431f-8b72-de46cd4d01d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4.4: BoAW encode each file using the loaded data (5%)\n",
    "Using the result of the K-means method and LLD_data, use your implementation of BoAW to build one histogram per utterance/file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1c8e5-95f1-4512-940c-e1619afc3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can collect all boaw representations in one matrix. We will later split it!\n",
    "\n",
    "def boaw_representations(LLD_data, scaler, pca_W, p_pca, K, M):\n",
    "    \"Generate matrix with one boaw representation (for each utterance) per row.\"\n",
    "    N_files = len(LLD_data) # One sample per file/utterance, each sample being a histogram.\n",
    "    #print(f\"Number of utterances: {N_files}\")\n",
    "    X_boaw_all = np.zeros((N_files, K)) # Each row will have K entries, each with the frequency of one of the clusters in that utterance/audio file.\n",
    "\n",
    "    for i in range(N_files):\n",
    "        #--- YOUR CODE HERE ---#\n",
    "        \n",
    "    return X_boaw_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330a4fe-3df6-4a0d-8aad-31f7936bef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "X_boaw_all = boaw_representations(LLD_data, scaler, pca_W, p_pca, K, M)\n",
    "print(f\"X_boaw_all shape: {X_boaw_all.shape}\")\n",
    "assert X_boaw_all.shape == (704, 256), \"Something went wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2fb50-3d21-48c2-a56a-ba04e6afe4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "# And now we split the representations in train and val sets\n",
    "\n",
    "def split_ravdess_filewise_boaw(X_boaw_all, mask_train, mask_val, mask_test):\n",
    "    \"\"\"Use the masks computed before to split X_boaw_all in train, val and test parts.\"\"\"\n",
    "    X_boaw_train = X_boaw_all[mask_train]\n",
    "    X_boaw_val = X_boaw_all[mask_val]\n",
    "    X_boaw_test = X_boaw_all[mask_test]\n",
    "    \n",
    "    return X_boaw_train, X_boaw_val, X_boaw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283fa79e-3d90-4cb5-9a12-23374ed050dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "X_boaw_train, X_boaw_val, X_boaw_test = split_ravdess_filewise_boaw(\n",
    "    X_boaw_all, mask_train, mask_val, mask_test\n",
    ")\n",
    "Y_train = emoids_train\n",
    "Y_val = emoids_val\n",
    "Y_test = emoids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64698d-0a6a-4f06-a6d6-0e08b1e4147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "print(X_boaw_train.shape)\n",
    "print(X_boaw_val.shape)\n",
    "print(X_boaw_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a61f1-c4d1-4923-a417-473c03f3ea31",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4.5: (5%)\n",
    "Use the sklearn implementation of LinearSVC (linear support vector machine) to optimize C using the validation set.\n",
    "\n",
    "To optimize C, simply try C = 10^-6, 10^-5, 10^-4, 10^-3, 10^-2, 10^-1, 10^-0, and select the one giving the best accuracy.\n",
    "Make sure to output best performance (i.e. accuracy) and corresponding C per BoAW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bc7d8-1b69-4c33-9b75-0933a9bb340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "C_set = 10.**np.arange(-6, 1) # the experimental set of SVM complexity values\n",
    "\n",
    "def generate_performances(X_train, X_val, Y_train, Y_val, C_set):\n",
    "    \"\"\"Compute the performances of linear svc (with random state = 1) for each C in C_set.\n",
    "    \n",
    "    Output:\n",
    "        performances -- (len(C_set), 1) array with accuracy corresponding to C_set[i] on entry i.\n",
    "        \n",
    "    \"\"\"\n",
    "    performances = np.zeros((len(C_set), 1)) # Performace for a certain C will simply be average correct preds = accuracy\n",
    "    for i in range(len(C_set)):\n",
    "        #--- YOUR CODE HERE ---#\n",
    "        \n",
    "    return performances\n",
    "    \n",
    "def find_best_C(C_set, performances):\n",
    "    \"\"\"Determines the best C in C_set using the list performances of accuracies.\n",
    "    \n",
    "    Output:\n",
    "        best_C_boaw -- float \n",
    "        best_accuracy_boaw -- float \n",
    "    \"\"\"\n",
    "    #--- YOUR CODE HERE ---#\n",
    "    \n",
    "    return best_C_boaw, best_accuracy_boaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef091c-7e22-4c92-b9d3-5c85b823472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "performances = generate_performances(X_boaw_train, X_boaw_val, Y_train, Y_val, C_set)\n",
    "best_C_boaw, best_accuracy_boaw = find_best_C(C_set, performances)\n",
    "\n",
    "print(f\"For K={K} and p_pca={p_pca}:\")\n",
    "print(f\"\\tBest accuracy: {best_accuracy_boaw}\")\n",
    "print(f\"\\tBest C: {best_C_boaw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afe09a-4bf8-4349-ae0b-076f2888df68",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4.5: Repeat steps 4.3 and 4.4 with different values of `p_pca` and `K` (0% - we do it for you!)\n",
    "\n",
    "Use `p_pca` in {120, 130}.\n",
    "Use `K` in {256, 512}.\n",
    "(Therefore you'll have 4 BoAW representations in total).\n",
    "\n",
    "Important note: since there will be hundreds of thousands of LLD vectors\n",
    "it will be wise to subsample them before K-Means taking one every `n_subsample=4` frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc5a31-7d4f-47d2-acc9-9684a23f5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "def generate_boaw_representation(X_zn, p_pca, pca_W, K, n_subsample, scaler):\n",
    "    X_projected = project_onto_pcs(X_zn, pca_W, p_pca)\n",
    "    X_projected_subsample = subsample(X_projected, n_subsample)\n",
    "    M, _ = mykmeans(X_projected_subsample, K)\n",
    "    X_boaw_all = boaw_representations(LLD_data, scaler, pca_W, p_pca, K, M)\n",
    "    #print(X_boaw_all.shape)\n",
    "    return X_boaw_all\n",
    "\n",
    "def test_hyperparameters(\n",
    "    X_zn, Y_train, Y_val, p_pcas, pca_W, Ks, n_subsample, scaler, C_set\n",
    "):\n",
    "    X_boaw_all_dict = {}\n",
    "    for p_pca in p_pcas:\n",
    "        for K in Ks:\n",
    "            print(\"-\"*5 + f\"\\np_pca: {p_pca}, K: {K}\")\n",
    "            \n",
    "            # Generate the representation\n",
    "            X_boaw_all = generate_boaw_representation(\n",
    "                X_zn, p_pca, pca_W, K, n_subsample, scaler\n",
    "            )\n",
    "            \n",
    "            # Split boaw data\n",
    "            X_boaw_train, X_boaw_val, X_boaw_test = split_ravdess_filewise_boaw(\n",
    "                X_boaw_all, mask_train, mask_val, mask_test\n",
    "            ) \n",
    "\n",
    "            # One list of performances per representation\n",
    "            performances = generate_performances(\n",
    "                X_boaw_train, X_boaw_val, Y_train, Y_val, C_set\n",
    "            )\n",
    "            \n",
    "            # One best C and best accuracy per representation\n",
    "            best_C_boaw, best_accuracy_boaw = find_best_C(C_set, performances) \n",
    "            \n",
    "            print(f\"\\nFor K={K} and p_pca={p_pca}:\")\n",
    "            print(f\"\\tBest accuracy: {float(best_accuracy_boaw)}\")\n",
    "            print(f\"\\tBest C: {best_C_boaw}\")\n",
    "            \n",
    "            X_boaw_all_dict[(p_pca, K)] = X_boaw_all\n",
    "            \n",
    "    return X_boaw_all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec34590-8a8e-4a11-a29f-52db6712010a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "n_subsample = 4\n",
    "p_pcas = [120, 130]\n",
    "Ks = [256, 512]\n",
    "X_boaw_all_dict = test_hyperparameters(\n",
    "    X_zn, Y_train, Y_val, p_pcas, pca_W, Ks, n_subsample, scaler, C_set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef577a-45f3-43ec-b40b-904e2ee93ed8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4.6: (0% - we do it for you!)\n",
    " We did everything using the train and validation sets only until now. This enables us to choose the best hyperparameters `p_pca`, `K` and `C`.\n",
    " \n",
    " Looking at the performance/accuracy results you obtained in the previous step, answer the question: which `p_pca`, `K` and `C` combination gave the best validation set performance?\n",
    " AND: using the boaw encoding for that `p_pca, K` combination and the best C, finally apply the setting on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3261a-02e2-4bf6-88ae-c1a35e39c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT GRADED, BUT YOU CAN PLAY WITH THE VALUES OF K, p_pca AND C\n",
    "# Best values:\n",
    "K = 512\n",
    "p_pca = 130\n",
    "C = 10.**-6\n",
    "\n",
    "# 1. Now we combine the training and validation, retrain a model with best C and\n",
    "# best boaw representation (i.e. best K, p_pca) then predict on the test set.\n",
    "\n",
    "# Generate boaw representation for this p_pca and K:\n",
    "X_boaw_all = X_boaw_all_dict[(p_pca, K)]\n",
    "\n",
    "# Split boaw data\n",
    "X_boaw_train, X_boaw_val, X_boaw_test = split_ravdess_filewise_boaw(\n",
    "    X_boaw_all, mask_train, mask_val, mask_test\n",
    ") \n",
    "\n",
    "\n",
    "# join train and val in one\n",
    "X_boaw_trainval = np.concatenate([X_boaw_train, X_boaw_val])\n",
    "Y_trainval = np.concatenate([Y_train, Y_val])\n",
    "\n",
    "# 2. Now we run the classification model\n",
    "clf = LinearSVC(C=C, random_state=1)\n",
    "clf.fit(X_boaw_trainval, Y_trainval)\n",
    "preds_test = clf.predict(X_boaw_test)\n",
    "performance = np.mean(Y_test == preds_test)\n",
    "\n",
    "print(f\"\\nFor K={K}, p_pca={p_pca} and C={C}:\")\n",
    "print(f\"\\tAccuracy: {performance}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc7e89-3233-443c-b08d-1bcf23946a79",
   "metadata": {},
   "source": [
    "Note: If everything went well, you should have an accuracy around 0.43 (on the test set) when using the best hyper-parameters `K`, `p_pca` and `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474db641-f497-4736-8609-0e939f262de9",
   "metadata": {},
   "source": [
    "**Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**\n",
    "\n",
    "--- End of the assignment ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8fc55d22b506f310823b6bc9ecb5d19dc173fb1373e7bf8bb90a2220d0a6ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
